# Git Issue Agent - Ollama configuration
#
# Uses a local Ollama instance for LLM inference.
# Prerequisite: Ollama must be running with the model pulled:
#   ollama pull ibm/granite4:latest

# LLM configuration
TASK_MODEL_ID=ollama/ibm/granite4:latest
LLM_API_BASE=http://ollama.ollama.svc:11434
LLM_API_KEY=ollama
MODEL_TEMPERATURE=0

# Agent service
SERVICE_PORT=8000
LOG_LEVEL=DEBUG

# MCP Tool endpoint
MCP_URL=http://github-tool-mcp:9090/mcp

# Keycloak / AuthBridge token settings
JWKS_URI=http://keycloak-service.keycloak.svc:8080/realms/demo/protocol/openid-connect/certs
TOKEN_URL=http://keycloak-service.keycloak.svc:8080/realms/demo/protocol/openid-connect/token
TARGET_SCOPES=github-full-access github-partial-access
